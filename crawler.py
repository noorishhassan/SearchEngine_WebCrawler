# -*- coding: utf-8 -*-
"""Copy of CS 449 IR A1 Crawler .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19nnFRhdaoDh8rKuI7gDZadAgRQftN1o-
"""

import os 
import random
from queue import Queue
import requests
import bs4
from urllib.parse import urlparse
from urllib.parse import urljoin
from urllib.parse import urldefrag
import urllib.robotparser
import numpy as np
import time
from queue import PriorityQueue
import threading
# Add any library to be imported here

# Crawler Parameters
BACKQUEUES= 3
THREADS= BACKQUEUES*3
FRONTQUEUES= 5
WAITTIME= 15 ; # wait 15 seconds before fetching URLS from 
PROBABILITYSUM = 15
SEEDURLS = ['https://docs.oracle.com/en/', 'https://www.oracle.com/corporate/', 'https://en.wikipedia.org/wiki/Machine_learning', 'https://www.csie.ntu.edu.tw/~cjlin/libsvm/index.html', 'https://docs.oracle.com/middleware/jet210/jet/index.html', 'https://en.wikipedia.org/w/api.php', 'https://en.wikipedia.org/api/', 'https://en.wikipedia.org/wiki/Weka_(machine_learning)']
ALLURLS = set([x for x in SEEDURLS])
# Add any other global parameters here

"""#**Frontier**
Frontier should use the Mercator frontier design as discussed in lecture.

Preferably it should be a class and should have the given functions.

*prioritizer* function is a stub right now, it will return a random number between 1 to f for given URL
"""

class frontier:
  seed_urls = SEEDURLS
  frontqueues = []
  backqueues = []
  backqueue_domains = {}
  schedule_heap = PriorityQueue()
  
# add the code for frontier here
# should have functions __init__, get_URL, add_URLs, add_to_backqueue
  def __init__(self):
    self.frontqueues = [Queue() for _ in range(FRONTQUEUES)]
    self.backqueues = [Queue() for _ in range(BACKQUEUES)]
    for x in range(BACKQUEUES):
      self.schedule_heap.put((x,time.time()))
    for x in self.seed_urls:
      self.add_to_frontqueue(x)
    for x in range(BACKQUEUES):
      self.add_to_backqueue(x)

  def add_URLs(self, suburls):
    print ("add_URLs adding URLs to front queues: ")
    for i,x in enumerate (suburls):
      self.add_to_frontqueue(x)
    #print ("add_URLs added URLs to front queues")

  def add_to_frontqueue(self, URL):
    #print ("added to front queues: ", URL)
    index=self.prioritizer(URL, FRONTQUEUES-1)
    self.frontqueues[index].put(URL)

  def add_to_backqueue(self, empty_backqueue):
    webpage = self.get_from_frontqueue()
    #print('webpage',webpage)
    domain = urlparse(webpage).netloc
    #print('domain', domain)
    if not domain in self.backqueue_domains:
      self.backqueue_domains[domain] = empty_backqueue
      self.backqueues[empty_backqueue].put(webpage)
    else:
      bq_number = self.backqueue_domains[domain]
      self.backqueues[bq_number].put(webpage)
    #print ("add_to_backqueue added ", webpage , " to backqueue")


  def get_from_frontqueue(self):
    frontqueue_selection = [x for x in range(FRONTQUEUES)]
    probabilities = self.compute_probability()
    selected_frontqueue = np.random.choice(frontqueue_selection,p = probabilities)

    while self.frontqueues[selected_frontqueue].empty():
      selected_frontqueue = np.random.choice(frontqueue_selection,p = probabilities)
      #print('selected frontqueue: ', selected_frontqueue)

    url = self.frontqueues[selected_frontqueue].get()
    print('url from frontqueue: ', url)
    return url

  def remove_from_dictionary(self, url):
    domain = urlparse(url).netloc
    del self.backqueue_domains[domain]

  def get_from_backqueue(self):
    # print ("get_from_backqueue getting URLs from back queues")

    x,backqueue_time = self.schedule_heap.get() 
    current_time = time.time()
    # print("got from heap")
    url = self.backqueues[x].get()

    if self.backqueues[x].empty():
      self.remove_from_dictionary(url)
      self.add_to_backqueue(x)
    
    if current_time - backqueue_time >= WAITTIME:
      print('time >=15')
    else:
      time.sleep((current_time - backqueue_time))
    
    print ("get_from_backqueue got URLs from back queues: ", url)
    self.schedule_heap.put((x, time.time()))
    return url


  def compute_probability(self):
    probability_sum = PROBABILITYSUM 
    p = []
    for x in range(FRONTQUEUES):
      p.append((x+1)/probability_sum)
    return p

  def prioritizer(self, URL,f):
    """
    Take URL and returns priority from 1 to F
    Right now it like a stub function. 
    It will return a random number from 1 to f for given inputs. 
    """
    return random.randint(1,f)

"""#**Fetch, Parse and Filter URLS**
Filter the URLS that are in robots.txt files of server and the have been already processed.
"""

# fetch
def fetch(URL):
  #get webpage
  webpage = requests.get(URL)
  #get webpage contents
  webpage_content = bs4.BeautifulSoup(webpage.text) 
  file=open('URL', '+w')
  file.write(webpage_content.get_text())
  return webpage_content

# parse
def parse(webpage_content, URL):
  suburls = []
  for link in webpage_content.find_all("a"):
    suburls.append(link.get("href"))
  http = 'http://'
  domain = urlparse(URL).netloc
  domain = http + domain

  for x,i in enumerate (suburls):
    suburls[x]=urljoin(domain, suburls[x])
  
  for x,i in enumerate (suburls):
    suburls[x]=urldefrag(suburls[x])[0]

  return suburls, domain

# filter
def filter(suburls, domain):
  robots = domain + '/robots.txt'
  
  rp = urllib.robotparser.RobotFileParser()
  rp.set_url(robots)
  rp.read()
  rp.crawl_delay("*")

  for x,i in enumerate (suburls):
    if not rp.can_fetch("*", suburls[x]):
      del suburls[x]

  for x,i in enumerate (suburls):
    if suburls[x] in ALLURLS:
      del suburls[x]

  return list(set(suburls)) # returns unique urls



"""#**Run Crawler**"""

class myThread (threading.Thread):
  def __init__(self, threadID, name):
    threading.Thread.__init__(self)
    self.threadID = threadID
    self.name = name
  def run(self):
    while(True):
      try:
        url = f.get_from_backqueue()
        print("crawling: ", url)
        webpage_content = fetch(url)
        suburls, domain = parse(webpage_content, url)
        suburls = filter(suburls, domain)
        f.add_URLs(suburls)
        ALLURLS.add(url)
      except Exception as e:
        print (e)
        continue

f=frontier()

threadLock = threading.Lock()
# Create new threads
threads = [myThread(x, f"Thread{x}") for x in range(THREADS)]

# Start new Threads
for x in range(THREADS):
  threads[x].start()

# Wait for all threads to complete
for t in threads:
    t.join()